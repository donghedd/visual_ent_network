<!--
 * @Author       : Li
 * @Date         : 2021-10-11 16:22:59
 * @LastEditTime : 2021-11-11 14:07:32
 * @LastEditors  :
 * @Description  :
 * @FilePath     : /visual_ent_network/README.md
-->

# 程序执行流程

- [程序执行流程](#程序执行流程)
  - [2021.10.12 更新](#20211012-更新)
  - [2021.10.27](#20211027)
  - [2021.10.28](#20211028)
    - [注意事项：](#注意事项)

1. 使用 LR-CNN 对输入的文本进行预测，最终产生一个 test_pred.csv 文件。
2. 使用 data_transform.py 对数据进行格式化处理（按照可视化的数据格式需求，生成 ents.json 文件）
3. 使用 jaal 工具预测结果进行可视化（读取 ents.json 数据文件）

4. 使用 Data_transform 中的 format_raw_text2conll 函数将纯文本（raw_text.txt）转换为可以预测的 conll 格式；
5. 执行 sh test.sh 对 pred_input 中的文本内容进行实体预测；预测结果为 test_pred.csv 文件；
6. 使用 Data_transform 中的\*\*函数生成 ents2.json 文件，该文件支持 jaal 可视化格式；
7. 执行 jaal_visualization.py 进行可视化操作；

## 2021.10.12 更新

1. 使用统一的调度入口（interface.py）来实现可视化，而不是分步骤执行（合并前面的步骤）
   > note: 如果可视化数据文件存在（ents2.json），那么久不会调用模型去自动提取。

## 2021.10.27

TODO List

[ ] 依据输入的文件名称，指定输出网页的名称；

[ ] 表格样式对 graph 进行基本描述：实体类型的解释；以及它们对应的 degree 信息；

- 描述文本总字数；
- 描述实体类型出现的次数，比如涵盖了多少概念，多少术语等；
- 依据\*\*论文的计算标准，对论文计算简单评分结果；

## 2021.10.28

- 在主页上，输入文件名和需要结构化的文本内容；
- 等待，经过 1-2 分钟的等待；
- 在结果页面呈现可视化的分析结果；（主要是采用网络图谱可视化的形式来呈现结果）

### 注意事项：

模型不能很好的处理英文语料，因为英文语料中存在 non-ASCII 编码的文字，会导致模型异常。

## 2022.08
### 增加文本行内高亮displacy工具可视化
1. 将模型预测的结果转换成CONLL格式；
2. 使用Spacy模块的displacy对CoNLL格式的预测结果进行文本行内高亮显示；
3. 通过URL传递当前文件名，在vis_text模块对CONLL格式的结果进行可视化；


### 增加了最大最小贡献度指标
> 通过添加最大、最小贡献度指标，使用者可以比较清楚的看到当前文档的一个质量水平；
1. 新增了update_contribution模块；
2. 归类template下的模板文件，1+name的文件均为程序的模板文件不能动；
3. 优化质量量化计算指标界面的overflow问题；设置style="height:**px;overflow:scroll"
4. 随机加载文档进行比对，引发认知冲突：
  - 实现路线1：传入一个文档名数组到jinja2模板，通过{{doc_set|random}} 随机从doc_set数组中产生一个文档名；定义一个全局变量来获取文件名，然后点击后，跳转到该随机文件；每次更新需要刷新本页面；
  - 实现路线2：在second_flask路由文件，直接生成随机的文件名；

> 注意事项：
> 如果templates下面存在某个文档名，但是在data目录下没有对应的文档文件夹，程序会报错，需要注意；

#### 重大bug：输入的句子长度过长，预测中会截短序列长度；
1. 由于句子的序列长度不能超过250长度，所以在准备预测的句子的时候需要对输入的句子序列进行整理，确保长度不超过250；
  1. 在data_transform.py中添加了一个cut_limit_sent函数来分割长句序列，设置的最大长度是249； [已解决]


### 并发处理的问题

1. 如果多个人同时输入问题，进行分析，程序会报错。因为大家都会去读raw_text, pred_input, test_pred这三个文件。
    - 解决办法：对这三个文件进行重命名，保存为用户输入的那个文件名，存放在data下面对应的目录中；[实现]
    - 解法2：重新复制一个目录，在同一台机器部署2个server，使用nginx代理两个不同的端口；并发较大，两个server产生结果都会比较慢慢。[实现]

2. 对里面的代码进行重构；


#### 
> 2022.09.03
1. 通过summary.ini来记录那个小组的文档被分析的时间戳，及频次；
2. 重新设置了total_ents.json文件，该文件用以计算相对贡献度；

> 2022.10.20 updates
1. 更新了cal_ents.py中的函数，之前pandas读取数据的结果是series格式，该格式转换为int的过程会出现异常。现在重新优化该部分内容；

#### Debug
1. 如果调用脚本，启动gunicorn出现问题。可以使用guniron命令启动，来查看错误信息；

> 2022.12.10 updates
1. 重写`cal_ents.py`文件中的count_words函数，可以迭代的从data下的子文件目录中读取.txt文件，统计它们的字数。这样可以避免输入的文本太短，出现一些非预期的“贡献度”结果。


#### Bug
1. 如果data目录下只有一个数据文件，会导致文内显示中随机加载的功能报错。这个地方需要添加验证逻辑；
2. 输入到首页文本框中的纯文本需要进一步校验里面的引号等符号，目前引号会导致数据处理的结果报错；"# visual_ent_network" 
"# visual_ent_network" 
